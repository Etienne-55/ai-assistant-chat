services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: agentic-backend
    networks:
     - agentic-network
    ports:
      - "3001:3001"
    environment:
      - PORT=3001
      - OLLAMA_BASE_URL=http://ollama:11434
      - NODE_ENV=production
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: agentic-frontend
    networks:
     - agentic-network
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=http://localhost:3001
    depends_on:
      - backend
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    networks:
      - agentic-network
    ports:
      - "11434:11434"
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
                

  ollama-setup:
    image: ollama/ollama:latest
    networks:
      - agentic-network
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Checking if models are already installed..."
        if ollama list | grep -q "qwen2.5:1.5b-instruct-q4_K_M"; then
          echo "Models already installed, skipping..."
          exit 0
        fi
        echo "Installing qwen2.5:1.5b-instruct-q4_K_M (this may take a few minutes on first run)..."
        ollama pull qwen2.5:1.5b-instruct-q4_K_M
        echo "âœ“ Model ready!"
    restart: "no"

networks:
  agentic-network:
    driver: bridge

volumes:
  ollama_data:
    driver: local

